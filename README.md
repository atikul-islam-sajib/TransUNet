# **ğŸš€ TransUNet - Unofficial Implementation of Transformer-Based U-Net for Medical Image Segmentation**

This is an **unofficial implementation** of **[TransUNet](https://arxiv.org/abs/2102.04306)**:  
> **"Transformers Make Strong Encoders for Medical Image Segmentation"**  
> *Jieneng Chen, Yutong Lu, Qihang Yu, et al.*

TransUNet is a **hybrid deep learning model** that combines **CNNs (Convolutional Neural Networks)** and **Transformers** to improve segmentation accuracy in medical imaging. This repository provides a **fully configurable** and **easy-to-use** implementation of TransUNet.

---

## **ğŸ“Œ Key Features**
âœ… **Unofficial Implementation** based on the **original paper**  
âœ… Hybrid **CNN + Transformer** architecture for enhanced segmentation  
âœ… Supports multiple optimizers: **Adam, AdamW, SGD**  
âœ… Configurable loss functions: **BCE, Focal, Tversky**  
âœ… Multi-device support: **CPU, GPU, Apple M1/M2 (`mps`)**  
âœ… Automatic **dataset handling, model training, and evaluation**  
âœ… Saves **segmentation masks** and **best-performing model checkpoints**  

---

## **ğŸ“Œ Model Architecture**
The **TransUNet** model follows a two-stage approach:  
1ï¸âƒ£ **CNN Encoder** - Extracts local spatial features  
2ï¸âƒ£ **Transformer Block** - Captures **global dependencies**  
3ï¸âƒ£ **Decoder** - Combines CNN and Transformer outputs for segmentation  

```
Input Image (H x W x C)
       â”‚
       â–¼
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ CNN Encoder â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   (Extracts feature maps)
       â”‚
       â–¼
-----> Transformer Block ----->   (Captures long-range dependencies)
       â”‚
       â–¼
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Decoder (UpSampling + Skip Connections) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
       â”‚
       â–¼
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Segmentation Mask (Output) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
```

---

## **ğŸ“Œ Installation**
### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/atikul-islam-sajib/TransUNet.git
cd TransUNet
```

### **2ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

### **3ï¸âƒ£ (Optional) Create a Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

---

## **ğŸ“Œ Configuration - `config.yaml`**
Before running training or testing, modify `config.yaml` to set paths, model parameters, and training options.

```yaml
# ğŸ“Œ Configuration File for TransUNet
# This file defines paths, data settings, model architecture, training parameters, and inference settings.

# ğŸ”¹ Paths for storing raw data, processed data, model checkpoints, and outputs
artifacts:
  raw_data_path: "./data/raw/"                         # Directory for raw dataset files
  processed_data_path: "./data/processed/"             # Directory for preprocessed dataset
  files_path: "./artifacts/files/"                     # General storage for generated files
  train_models: "./artifacts/checkpoints/train_models/"  # Directory to store trained models
  best_model: "./artifacts/checkpoints/best_model/"      # Directory for best model checkpoints
  metrics_path: "./artifacts/metrics/"                 # Path to store training/testing metrics
  train_images: "./artifacts/outputs/train_images/"    # Folder to store images generated during training
  test_image: "./artifacts/outputs/test_image/"        # Folder to store predicted test images

# ğŸ”¹ Dataset and dataloader settings
dataloader:
  image_path: "./data/raw/dataset.zip"  # Path to the dataset (ZIP format or unzipped folder)
  image_channels: 3                     # Number of image channels (3 for RGB, 1 for grayscale)
  image_size: 128                        # Image resolution (e.g., 128x128)
  batch_size: 8                          # Number of images per batch
  split_size: 0.30                       # Percentage of data used for validation (e.g., 30% validation)

# ğŸ”¹ TransUNet Model Configuration
TransUNet:
  nheads: 4              # Number of attention heads in the transformer encoder
  num_layers: 4          # Number of transformer encoder layers
  dim_feedforward: 512   # Hidden layer size in the feedforward network
  dropout: 0.3           # Dropout rate for regularization (higher value prevents overfitting)
  activation: "gelu"     # Activation function ("gelu" or "relu")
  layer_norm_eps: 1e-05  # Epsilon value for layer normalization (stabilizes training)
  bias: False            # Whether to use bias in transformer layers (True/False)

# ğŸ”¹ Training Configuration
trainer:
  epochs: 100            # Number of epochs for training
  lr: 0.0001             # Learning rate for optimization
  optimizer: "AdamW"     # Selected optimizer: "Adam", "AdamW", or "SGD"

  # Optimizer configurations (fine-tuning parameters)
  optimizers:
    Adam: 
      beta1: 0.9
      beta2: 0.999
      weight_decay: 0.0001
    SGD: 
      momentum: 0.95
      weight_decay: 0.0
    AdamW:
      beta1: 0.9
      beta2: 0.999
      weight_decay: 0.0001

  # Loss function settings
  loss: 
    type: "bce"           # Type of loss function: "bce", "focal", or "tversky"
    loss_smooth: 1e-06    # Smoothing factor for loss computation (prevents overconfidence)
    alpha_focal: 0.75     # Alpha value for focal loss (balances class distribution)
    gamma_focal: 2        # Gamma value for focal loss (higher values focus on hard examples)
    alpha_tversky: 0.75   # Alpha parameter for Tversky loss (controls false positives)
    beta_tversky: 0.5     # Beta parameter for Tversky loss (controls false negatives)

  # Regularization settings (helps prevent overfitting)
  l1_regularization: False       # Enable L1 regularization (True/False)
  elastic_net_regularization: False  # Enable elastic net regularization (True/False)

  verbose: True       # Display progress and save images during training (True/False)
  device: "cuda"      # Device for training: "cuda" (GPU), "mps" (Mac M1/M2), or "cpu"

# ğŸ”¹ Testing Configuration
tester:
  dataset: "test"  # Dataset used for testing
  device: "cuda"   # Device to use for testing: "cuda", "mps", or "cpu"

# ğŸ”¹ Inference Configuration
inference:
  image: "./artifacts/data/processed/sample.jpg"  # Path to the image used for inference
```

---

### **ğŸ“Œ Explanation of Key Sections**
| **Section**       | **Description** |
|-------------------|----------------|
| **`artifacts`**  | Defines storage paths for datasets, model checkpoints, and outputs. |
| **`dataloader`** | Specifies dataset path, image properties, batch size, and validation split. |
| **`TransUNet`**  | Defines model architecture, including Transformer layers and activation functions. |
| **`trainer`**    | Configures training parameters like epochs, optimizer, and loss functions. |
| **`tester`**     | Specifies dataset and device settings for evaluation. |
| **`inference`**  | Defines the path to an image for making predictions. |
---

## **ğŸ“Œ Running Training & Testing**
| **Process**      | **Command**                     | **Description** |
|------------------|---------------------------------|-----------------|
| **Train Model**  | `python src/cli.py --train`     | Starts model training using `config.yaml` |
| **Test Model**   | `python src/cli.py --test`      | Runs evaluation on test data |
| **Change Optimizer** | Edit `config.yaml`          | Supports `"SGD"`, `"Adam"`, `"AdamW"` |

---

## **ğŸ“Œ Viewing Results**
### **ğŸ”¹ Model Checkpoints**
Trained models are saved in:
```bash
./artifacts/checkpoints/train_models/
```

### **ğŸ”¹ Best Model**
The best-performing model is saved in:
```bash
./artifacts/checkpoints/best_model/
```

### **ğŸ”¹ Test Predictions**
Generated segmentation masks are saved in:
```bash
./artifacts/outputs/test_image/
```

---

## **ğŸ“Œ TransUNet Workflow**
```
1ï¸âƒ£ Load Dataset  ----->  2ï¸âƒ£ Train Model  ----->  3ï¸âƒ£ Evaluate  ----->  4ï¸âƒ£ Generate Predictions
```

---

## **ğŸ“Œ Citation**
If you use this repository, please consider citing the **original TransUNet paper**:
```
@article{chen2021transunet,
  title={TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation},
  author={Chen, Jieneng and Lu, Yutong and Yu, Qihang and others},
  journal={arXiv preprint arXiv:2102.04306},
  year={2021}
}
```

---

## **ğŸ“Œ License**
This project is **open-source** and available under the **MIT License**.  
ğŸš¨ **Note:** This is an **unofficial implementation** and is **not affiliated** with the original authors.

---

# **ğŸ”¥ Ready to Get Started?**
- Modify `config.yaml` for your setup.  
- Run **training** with: `python src/cli.py --train`  
- Run **testing** with: `python src/cli.py --test`  

ğŸš€ **Happy Training & Testing with TransUNet!** ğŸš€  