{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import yaml\n",
    "import torch\n",
    "import joblib\n",
    "import zipfile\n",
    "import warnings\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchview import draw_graph\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_files():\n",
    "    with open(\"../notebook_config.yml\", \"r\") as config_file:\n",
    "        return yaml.safe_load(config_file)\n",
    "\n",
    "\n",
    "def dump_files(value=None, filename=None):\n",
    "    if (value is not None) and (filename is not None):\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Determine the filename and value of a config file\".capitalize()\n",
    "        )\n",
    "\n",
    "\n",
    "def load_file(filename: str = None):\n",
    "    if filename is not None:\n",
    "        return joblib.load(filename)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Please provide a filename to load config data from.\".capitalize()\n",
    "        )\n",
    "\n",
    "\n",
    "def device_init(device: str = \"cuda\"):\n",
    "    if device == \"cuda\":\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    elif device == \"mps\":\n",
    "        return torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    else:\n",
    "        return torch.device(device)\n",
    "\n",
    "\n",
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find(\"BatchNorm\") != -1:\n",
    "            nn.init.constant_(m.weight.data, 1.0)\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def path_names():\n",
    "    raw_data_path: str = config_files()[\"artifacts\"][\"raw_data_path\"]\n",
    "    processed_data_path: str = config_files()[\"artifacts\"][\"processed_data_path\"]\n",
    "    train_models_path: str = config_files()[\"artifacts\"][\"train_models\"]\n",
    "    best_model_path: str = config_files()[\"artifacts\"][\"best_model\"]\n",
    "    files_path: str = config_files()[\"artifacts\"][\"files_path\"]\n",
    "    metrics_path: str = config_files()[\"artifacts\"][\"metrics_path\"]\n",
    "    train_images: str = config_files()[\"artifacts\"][\"train_images\"]\n",
    "    test_image: str = config_files()[\"artifacts\"][\"test_image\"]\n",
    "    image_path: str = config_files()[\"dataloader\"][\"image_path\"]\n",
    "\n",
    "    return {\n",
    "        \"raw_data_path\": raw_data_path,\n",
    "        \"processed_data_path\": processed_data_path,\n",
    "        \"train_models_path\": train_models_path,\n",
    "        \"best_model_path\": best_model_path,\n",
    "        \"files_path\": files_path,\n",
    "        \"metrics_path\": metrics_path,\n",
    "        \"train_images\": train_images,\n",
    "        \"test_image\": test_image,\n",
    "        \"image_path\": image_path,\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_images(\n",
    "    predicted_images: torch.Tensor = None,\n",
    "    predicted: bool = False,\n",
    "):\n",
    "    processed_data_path = path_names()[\"processed_data_path\"]\n",
    "    processed_data_path = load_file(\n",
    "        filename=os.path.join(processed_data_path, \"test_dataloader.pkl\")\n",
    "    )\n",
    "    images, masks = next(iter(processed_data_path))\n",
    "\n",
    "    max_number = min(16, images.size(0))\n",
    "    num_of_rows = math.ceil(math.sqrt(max_number))\n",
    "    num_of_columns = math.ceil(max_number / num_of_rows)\n",
    "\n",
    "    plt.figure(figsize=(10, 20))\n",
    "\n",
    "    for index, (image, mask) in enumerate(zip(images[:max_number], masks[:max_number])):\n",
    "        image = image.squeeze().permute(1, 2, 0).detach().cpu().numpy()\n",
    "        mask = mask.squeeze().detach().cpu().numpy()\n",
    "\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "        mask = (mask - mask.min()) / (mask.max() - mask.min())\n",
    "\n",
    "        if predicted:\n",
    "            pred_mask = predicted_images[index].squeeze().permute(1, 2, 0)\n",
    "            pred_mask = pred_mask.detach().cpu().numpy()\n",
    "            pred_mask = (pred_mask - pred_mask.min()) / (\n",
    "                pred_mask.max() - pred_mask.min()\n",
    "            )\n",
    "\n",
    "            plt.subplot(3 * num_of_rows, num_of_columns, 3 * index + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Image\")\n",
    "\n",
    "            plt.subplot(3 * num_of_rows, num_of_columns, 3 * index + 2)\n",
    "            plt.imshow(mask, cmap=\"gray\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Mask\")\n",
    "\n",
    "            plt.subplot(3 * num_of_rows, num_of_columns, 3 * index + 3)\n",
    "            plt.imshow(pred_mask, cmap=\"gray\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Pred Mask\")\n",
    "\n",
    "        else:\n",
    "            plt.subplot(2 * num_of_rows, num_of_columns, 2 * index + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Image\")\n",
    "\n",
    "            plt.subplot(2 * num_of_rows, num_of_columns, 2 * index + 2)\n",
    "            plt.imshow(mask, cmap=\"gray\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Mask\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = os.path.join(path_names()[\"files_path\"], \"images.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(\"Image files saved in\", save_path)\n",
    "\n",
    "def total_params(model=None):\n",
    "    if model is None:\n",
    "        raise ValueError(\n",
    "            \"Please provide a model to calculate the total parameters.\".capitalize()\n",
    "        )\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def plot_model_architecture(\n",
    "    model=None,\n",
    "    input_data: torch.Tensor = None,\n",
    "    model_name: str = \"./artifacts/files_name/\",\n",
    "    format: str = \"pdf\",\n",
    "):\n",
    "    if model is None and not isinstance(input_data, torch):\n",
    "        raise ValueError(\n",
    "            \"Please provide a model and input data to plot the model architecture.\".capitalize()\n",
    "        )\n",
    "\n",
    "    filename = path_names()[\"files_path\"]\n",
    "    draw_graph(model=model, input_data=input_data).visual_graph.render(\n",
    "        filename=os.path.join(filename, model_name), format=format\n",
    "    )\n",
    "    print(f\"Model architecture saved in {filename}/{model_name}.{format}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_path=None,\n",
    "        image_channels: int = 3,\n",
    "        image_size: int = 224,\n",
    "        batch_size: int = 4,\n",
    "        split_size: float = 0.25,\n",
    "        shuffle: bool = False,\n",
    "    ):\n",
    "        self.image_channels = image_channels\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.split_size = split_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        if image_path is None:\n",
    "            self.image_path = path_names()[\"image_path\"]\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                \"Ensure the provided image path is correct; an incorrect path may result in an error.\".title()\n",
    "            )\n",
    "            self.image_path = image_path\n",
    "\n",
    "        self.train_images = list()\n",
    "        self.train_masks = list()\n",
    "        self.test_images = list()\n",
    "        self.test_masks = list()\n",
    "\n",
    "    def transform_images(self, type: str = \"image\"):\n",
    "        if type == \"image\":\n",
    "            return transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((self.image_size, self.image_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.CenterCrop((self.image_size, self.image_size)),\n",
    "                    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "                ]\n",
    "            )\n",
    "        elif type == \"mask\":\n",
    "            return transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((self.image_size, self.image_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.CenterCrop((self.image_size, self.image_size)),\n",
    "                    transforms.Normalize((0.5,), (0.5,)),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def unzip_folder(self):\n",
    "        if os.path.exists(self.image_path):\n",
    "            with zipfile.ZipFile(file=self.image_path, mode=\"r\") as zip_file:\n",
    "                zip_file.extractall(path=path_names()[\"processed_data_path\"])\n",
    "            print(\n",
    "                f\"Dataset unzipped successfully in {path_names()['processed_data_path']}\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Image file not found\".capitalize())\n",
    "\n",
    "    def features_extracted(self):\n",
    "        processed_path = path_names()[\"processed_data_path\"]\n",
    "        train_path = os.path.join(processed_path, \"dataset\", \"train\")\n",
    "        valid_path = os.path.join(processed_path, \"dataset\", \"test\")\n",
    "\n",
    "        train_images_path = os.path.join(train_path, \"image\")\n",
    "        train_masks_path = os.path.join(train_path, \"mask\")\n",
    "\n",
    "        test_images_path = os.path.join(valid_path, \"image\")\n",
    "        test_masks_path = os.path.join(valid_path, \"mask\")\n",
    "\n",
    "        for type, path in tqdm(\n",
    "            [(\"train\", train_images_path), (\"test\", test_images_path)],\n",
    "            desc=\"Extracted features ..\",\n",
    "        ):\n",
    "            try:\n",
    "                mask_path = train_masks_path if type == \"train\" else test_masks_path\n",
    "\n",
    "                for image in os.listdir(path):\n",
    "                    try:\n",
    "                        if image in os.listdir(mask_path):\n",
    "                            image_path = os.path.join(path, image)\n",
    "                            mask_image_path = os.path.join(mask_path, image)\n",
    "\n",
    "                            _image = cv2.imread(image_path)\n",
    "                            if _image is None:\n",
    "                                raise FileNotFoundError(\n",
    "                                    f\"Image file not found: {image_path}\"\n",
    "                                )\n",
    "                            _image = cv2.cvtColor(_image, cv2.COLOR_BGR2RGB)\n",
    "                            _image = Image.fromarray(_image)\n",
    "                            _image = self.transform_images(type=\"image\")(_image)\n",
    "\n",
    "                            _mask = cv2.imread(mask_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                            _mask = Image.fromarray(_mask)\n",
    "                            _mask = self.transform_images(type=\"mask\")(_mask)\n",
    "\n",
    "                            if _mask is None:\n",
    "                                raise FileNotFoundError(\n",
    "                                    f\"Mask file not found: {mask_image_path}\"\n",
    "                                )\n",
    "\n",
    "                            if type == \"train\":\n",
    "                                self.train_images.append(_image)\n",
    "                                self.train_masks.append(_mask)\n",
    "\n",
    "                            elif type == \"test\":\n",
    "                                self.test_images.append(_image)\n",
    "                                self.test_masks.append(_mask)\n",
    "\n",
    "                    except FileNotFoundError as e:\n",
    "                        print(f\"[WARNING] {e}\")\n",
    "                    except cv2.error as e:\n",
    "                        print(f\"[ERROR] OpenCV error while processing {image}: {e}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"[ERROR] Unexpected error while processing {image}: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[CRITICAL] Failed to process dataset for {type} images: {e}\")\n",
    "\n",
    "        assert len(self.train_images) == len(\n",
    "            self.train_masks\n",
    "        ), \"Images, Masks should be equal in size\".capitalize()\n",
    "\n",
    "        assert len(self.test_images) == len(\n",
    "            self.test_masks\n",
    "        ), \"Images, Masks should be equal in size\".capitalize()\n",
    "\n",
    "        return {\n",
    "            \"train_images\": self.train_images,\n",
    "            \"train_masks\": self.train_masks,\n",
    "            \"test_images\": self.test_images,\n",
    "            \"test_masks\": self.test_masks,\n",
    "        }\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        try:\n",
    "            dataset = self.features_extracted()\n",
    "\n",
    "            train_dataloader = DataLoader(\n",
    "                dataset=list(zip(dataset[\"train_images\"], dataset[\"train_masks\"])),\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=self.shuffle,\n",
    "            )\n",
    "\n",
    "            valid_dataloader = DataLoader(\n",
    "                dataset=list(zip(dataset[\"test_images\"], dataset[\"test_masks\"])),\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=self.shuffle,\n",
    "            )\n",
    "\n",
    "            for filename, value in [\n",
    "                (\"train_dataloader.pkl\", train_dataloader),\n",
    "                (\"test_dataloader.pkl\", valid_dataloader),\n",
    "            ]:\n",
    "                dump_files(\n",
    "                    value=value,\n",
    "                    filename=os.path.join(\n",
    "                        path_names()[\"processed_data_path\"], filename\n",
    "                    ),\n",
    "                )\n",
    "                print(f\"Dataloader saved as {filename}\".capitalize())\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"[WARNING] {e}\")\n",
    "        except cv2.error as e:\n",
    "            print(f\"[ERROR] OpenCV error while processing: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Unexpected error while processing: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def display_images():\n",
    "        try:\n",
    "            plot_images(predicted=False)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Unexpected error while displaying images: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def dataset_details():\n",
    "        processed_data_path = path_names()[\"processed_data_path\"]\n",
    "\n",
    "        train_dataloader = os.path.join(processed_data_path, \"train_dataloader.pkl\")\n",
    "        valid_dataloader = os.path.join(processed_data_path, \"test_dataloader.pkl\")\n",
    "\n",
    "        train_dataloader = load_file(filename=train_dataloader)\n",
    "        valid_dataloader = load_file(filename=valid_dataloader)\n",
    "\n",
    "        train_images, _ = next(iter(train_dataloader))\n",
    "        _, valid_masks = next(iter(valid_dataloader))\n",
    "\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Train Images\": sum(images.size(0) for images, _ in train_dataloader),\n",
    "                \"Valid Images\": sum(images.size(0) for images, _ in valid_dataloader),\n",
    "                \"Train Masks\": sum(masks.size(0) for _, masks in train_dataloader),\n",
    "                \"Valid Masks\": sum(masks.size(0) for _, masks in valid_dataloader),\n",
    "                \"Image Size\": str(train_images.size()),\n",
    "                \"Mask Size\": str(valid_masks.size()),\n",
    "            },\n",
    "            index=[\"Dataset Details\"],\n",
    "        ).to_csv(os.path.join(path_names()[\"files_path\"], \"dataset_details.csv\"))\n",
    "        print(f\"Dataset details saved to {path_names()['files_path']}\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = Loader(\n",
    "        image_path=\"../data/raw/dataset.zip\",\n",
    "        image_channels=3,\n",
    "        image_size=128,\n",
    "        batch_size=16,\n",
    "        split_size=0.25,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    loader.unzip_folder()\n",
    "    loader.create_dataloader()\n",
    "\n",
    "    Loader.display_images()\n",
    "    Loader.dataset_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "1. BCELoss\n",
    "2. DiceLoss\n",
    "3. JaccardLoss\n",
    "4. FocalLoss\n",
    "5. TverskyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCE(nn.Module):\n",
    "\n",
    "    def __init__(self, name: str = \"BCEWithLogitsLoss\", reduction: str = \"mean\"):\n",
    "        super(BCE, self).__init__()\n",
    "        self.name = name\n",
    "        self.reduction = reduction\n",
    "\n",
    "        if self.name == \"BCEWithLogitsLoss\":\n",
    "            self.loss_func = nn.BCEWithLogitsLoss(reduction=self.reduction)\n",
    "        elif self.name == \"BCELoss\":\n",
    "            self.loss_func = nn.BCELoss(reduction=self.reduction)\n",
    "\n",
    "    def forward(self, predicted: torch.Tensor, actual: torch.Tensor):\n",
    "        if isinstance(predicted, torch.Tensor) and isinstance(actual, torch.Tensor):\n",
    "            return self.loss_func(predicted, actual)\n",
    "        else:\n",
    "            raise ValueError(\"Input must be torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predicted = torch.Tensor([3.0, 2.0, -1.0, -3.0, 5.0])\n",
    "    actual = torch.Tensor([1.0, 1.0, 0.0, 0.0, 1.0])\n",
    "\n",
    "    loss = BCE(name=\"BCEWithLogitsLoss\", reduction=\"mean\")\n",
    "    assert type(loss(predicted, actual)) == torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, reduction: str = \"mean\", smooth: float = 1e-5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, predicted: torch.Tensor, actual: torch.Tensor):\n",
    "        if not isinstance(predicted, torch.Tensor) or not isinstance(\n",
    "            actual, torch.Tensor\n",
    "        ):\n",
    "            raise ValueError(\"Inputs must be torch.Tensor\")\n",
    "\n",
    "        predicted = torch.sigmoid(predicted)\n",
    "\n",
    "        predicted = predicted.view(predicted.shape[0], -1)\n",
    "        actual = actual.view(actual.shape[0], -1)\n",
    "\n",
    "        intersection = (predicted * actual).sum(dim=1)\n",
    "        union = predicted.sum(dim=1) + actual.sum(dim=1)\n",
    "\n",
    "        dice_score = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "\n",
    "        loss = 1 - dice_score\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predicted = torch.randn((64, 1, 128, 128))\n",
    "    actual = torch.randint(0, 2, (64, 1, 128, 128)).float()\n",
    "\n",
    "    loss_func = DiceLoss(reduction=\"mean\", smooth=1e-5)\n",
    "    print(loss_func(predicted, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaccardLoss(nn.Module):\n",
    "    def __init__(self, reduction: str = \"mean\", smooth: float = 1e-5):\n",
    "        super(JaccardLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, predicted: torch.Tensor, actual: torch.Tensor):\n",
    "        if not isinstance(predicted, torch.Tensor) or not isinstance(\n",
    "            actual, torch.Tensor\n",
    "        ):\n",
    "            raise ValueError(\"Inputs must be torch.Tensor\")\n",
    "\n",
    "        predicted = torch.sigmoid(predicted)\n",
    "\n",
    "        predicted = predicted.view(predicted.shape[0], -1)\n",
    "        actual = actual.view(actual.shape[0], -1)\n",
    "\n",
    "        intersection = (predicted * actual).sum(dim=1)\n",
    "        union = (predicted + actual - (predicted * actual)).sum(dim=1)\n",
    "\n",
    "        jaccard_score = (intersection + self.smooth) / (union + self.smooth)\n",
    "\n",
    "        loss = 1 - jaccard_score\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predicted = torch.randn((64, 1, 128, 128))\n",
    "    actual = torch.randint(0, 2, (64, 1, 128, 128)).float()\n",
    "\n",
    "    loss_func = JaccardLoss(reduction=\"mean\", smooth=1e-5)\n",
    "    print(loss_func(predicted, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.0, reduction=\"mean\"):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, predicted: torch.Tensor, actual: torch.Tensor):\n",
    "        if not isinstance(predicted, torch.Tensor) or not isinstance(\n",
    "            actual, torch.Tensor\n",
    "        ):\n",
    "            raise ValueError(\"Inputs must be torch.Tensor\")\n",
    "\n",
    "        bce_loss = self.bce_loss(predicted, actual)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pt = torch.exp(-bce_loss)\n",
    "\n",
    "        focal_weight = self.alpha * (1 - pt) ** self.gamma\n",
    "        focal_loss = focal_weight * bce_loss\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    predicted = torch.randn((64, 1, 128, 128))\n",
    "    actual = torch.randint(0, 2, (64, 1, 128, 128)).float()\n",
    "\n",
    "    loss_func = FocalLoss(alpha=0.75, gamma=2.0, reduction=\"mean\")\n",
    "\n",
    "    print(loss_func(predicted, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha: float = 0.7, beta: float = 0.3, reduction: str = \"mean\"):\n",
    "        super(TverskyLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, predicted: torch.Tensor, actual: torch.Tensor):\n",
    "        if not isinstance(predicted, torch.Tensor) or not isinstance(\n",
    "            actual, torch.Tensor\n",
    "        ):\n",
    "            raise ValueError(\"Inputs must be torch.Tensor\")\n",
    "\n",
    "        predicted = torch.sigmoid(predicted)\n",
    "\n",
    "        predicted = predicted.view(predicted.size(0), -1)\n",
    "        actual = actual.view(actual.size(0), -1)\n",
    "\n",
    "        TP = (predicted * actual).sum(dim=1)\n",
    "        FP = (predicted * (1 - actual)).sum(dim=1)\n",
    "        FN = ((1 - predicted) * actual).sum(dim=1)\n",
    "\n",
    "        epsilon = 1e-8\n",
    "        tversky_index = TP / (TP + self.alpha * FP + self.beta * FN + epsilon)\n",
    "\n",
    "        tversky_loss = 1 - tversky_index\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return tversky_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return tversky_loss.sum()\n",
    "        else:\n",
    "            return tversky_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predicted = torch.randn((64, 1, 128, 128))\n",
    "    actual = torch.randint(0, 2, (64, 1, 128, 128)).float()\n",
    "\n",
    "    loss_func = TverskyLoss(alpha=0.7, beta=0.3, reduction=\"mean\")\n",
    "    print(loss_func(actual, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product -> Self Attention Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "    if not (\n",
    "        isinstance(query, torch.Tensor)\n",
    "        and isinstance(key, torch.Tensor)\n",
    "        and isinstance(value, torch.Tensor)\n",
    "    ):\n",
    "        raise ValueError(\"Inputs must be torch.Tensor\")\n",
    "\n",
    "    key = key.transpose(-1, -2)\n",
    "\n",
    "    logits = torch.matmul(query, key)\n",
    "    dimension = key.size(-1)\n",
    "\n",
    "    logits = logits / torch.sqrt(\n",
    "        torch.tensor(float(dimension), dtype=query.dtype, device=query.device)\n",
    "    )\n",
    "    logits = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    attention = torch.matmul(logits, value)\n",
    "\n",
    "    return attention\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = torch.randn(1, 4, 256, 128)\n",
    "    key = torch.randn(1, 4, 256, 128)\n",
    "    value = torch.randn(1, 4, 256, 128)\n",
    "\n",
    "    attention = scaled_dot_product(query, key, value)\n",
    "    print(attention.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, nheads: int = 4, dimension: int = 256):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nheads = nheads\n",
    "        self.dimension = dimension\n",
    "\n",
    "        assert dimension % nheads == 0, \"Dimension must be divisible by nheads\"\n",
    "\n",
    "        self.QKV = nn.Linear(\n",
    "            in_features=self.dimension, out_features=3 * self.dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            QKV = self.QKV(x)\n",
    "\n",
    "            query, key, value = torch.chunk(input=QKV, chunks=3, dim=-1)\n",
    "\n",
    "            query = query.view(\n",
    "                query.size(0), query.size(1), self.nheads, self.dimension // self.nheads\n",
    "            )\n",
    "            key = key.view(\n",
    "                key.size(0), key.size(1), self.nheads, self.dimension // self.nheads\n",
    "            )\n",
    "            value = value.view(\n",
    "                value.size(0), value.size(1), self.nheads, self.dimension // self.nheads\n",
    "            )\n",
    "\n",
    "            query = query.permute(0, 2, 1, 3)\n",
    "            key = key.permute(0, 2, 1, 3)\n",
    "            value = value.permute(0, 2, 1, 3)\n",
    "\n",
    "            attention = scaled_dot_product(query=query, key=key, value=value)\n",
    "\n",
    "            attention = attention.view(\n",
    "                attention.size(0),\n",
    "                attention.size(2),\n",
    "                attention.size(1) * attention.size(3),\n",
    "            )\n",
    "            return attention\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    attention = MultiHeadAttention(nheads=4, dimension=512)\n",
    "    images = torch.randn(1, 256, 512)\n",
    "    print(attention(x=images).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Foward Neural Network : MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int = 512,\n",
    "        dim_feedforward: int = 1024,\n",
    "        activation: str = \"relu\",\n",
    "        dropout: float = 0.1,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "        self.dimension = dimension\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            self.activation_func = nn.ReLU(inplace=True)\n",
    "        elif self.activation == \"leaky\":\n",
    "            self.activation_func = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        elif self.activation == \"gelu\":\n",
    "            self.activation_func = nn.GELU(inplace=True)\n",
    "        elif self.activation == \"selu\":\n",
    "            self.activation_func = nn.SELU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\".capitalize())\n",
    "\n",
    "        self.layers = list()\n",
    "\n",
    "        for index in range(2):\n",
    "            self.layers.append(\n",
    "                nn.Linear(\n",
    "                    in_features=self.dimension,\n",
    "                    out_features=self.dim_feedforward,\n",
    "                    bias=self.bias,\n",
    "                )\n",
    "            )\n",
    "            self.dimension = self.dim_feedforward\n",
    "            self.dim_feedforward = dimension\n",
    "\n",
    "            if index == 0:\n",
    "                self.layers.append(self.activation_func)\n",
    "                self.layers.append(nn.Dropout(p=self.dropout))\n",
    "\n",
    "        self.network = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise ValueError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "        x = self.network(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    network = FeedForwardNeuralNetwork(dimension=512, dim_feedforward=1024)\n",
    "    images = torch.randn((1, 256, 512))\n",
    "    print(network(x=images).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, dimension: int = 512, layer_norm_eps: float = 1e-05):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        self.dimension = dimension\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "\n",
    "        self.alpha = nn.Parameter(\n",
    "            data=torch.ones((1, 1, self.dimension)), requires_grad=True\n",
    "        )\n",
    "        self.beta = nn.Parameter(\n",
    "            data=torch.zeros((1, 1, self.dimension)), requires_grad=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise ValueError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "        mean = torch.mean(input=x, dim=-1)\n",
    "        variance = torch.var(input=x, dim=-1)\n",
    "\n",
    "        mean = mean.unsqueeze(-1)\n",
    "        variance = variance.unsqueeze(-1)\n",
    "\n",
    "        normalized_x = (x - mean) / torch.sqrt(variance + self.layer_norm_eps)\n",
    "\n",
    "        return self.alpha * normalized_x + self.beta\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    layer_norm = LayerNormalization(dimension=512)\n",
    "    images = torch.randn((1, 256, 512))\n",
    "    print(layer_norm(x=images).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 128,\n",
    "        patch_size: int = 1,\n",
    "        dimension: int = 1024,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.dimension = dimension\n",
    "        self.bias = bias\n",
    "        self.constant = 16\n",
    "\n",
    "        assert (\n",
    "            self.image_size % self.patch_size == 0\n",
    "        ), \"Image size must be divisible by the patch size.\".title()\n",
    "\n",
    "        warnings.warn(\n",
    "            \"The encoder block extracts features, which may reduce the effective image size. \"\n",
    "            \"To mitigate this, we set the patch size to 1.\"\n",
    "        )\n",
    "\n",
    "        self.num_of_patches = (\n",
    "            (self.image_size // self.constant) // self.patch_size\n",
    "        ) ** 2\n",
    "        self.in_channels, self.out_channels = self.dimension, self.dimension\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.postitonal_embedding = nn.Parameter(\n",
    "            data=torch.randn((1, self.num_of_patches, self.dimension)),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = self.projection(x)\n",
    "            x = x.view(x.size(0), x.size(-1) * x.size(-2), x.size(1))\n",
    "            x = self.postitonal_embedding + x\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    patch_embedding = PatchEmbedding(image_size=128, patch_size=1, dimension=1024)\n",
    "    images = torch.randn((16, 1024, 8, 8))\n",
    "    print(patch_embedding(x=images).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block for TransUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int = 128, out_channels: int = 2 * 128):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.kernel_size = 1\n",
    "        self.stride_size = 1\n",
    "        self.padding_size = self.kernel_size // 2\n",
    "\n",
    "        self.layers = list()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.out_channels,\n",
    "                kernel_size=self.kernel_size,\n",
    "                stride=self.stride_size * 2,\n",
    "                padding=self.padding_size,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "        )\n",
    "\n",
    "        self.in_channels = self.in_channels\n",
    "        self.out_channels = self.out_channels\n",
    "\n",
    "        for index in range(3):\n",
    "            if index != 1:\n",
    "                self.kernel_size = self.kernel_size // self.kernel_size\n",
    "                self.stride_size = self.stride_size // self.stride_size\n",
    "                self.padding_size = self.kernel_size // 2\n",
    "            else:\n",
    "                self.kernel_size = self.kernel_size * 3\n",
    "                self.stride_size = self.stride_size * 2\n",
    "                self.padding_size = self.kernel_size // self.kernel_size\n",
    "\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=self.in_channels,\n",
    "                    out_channels=self.out_channels,\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    stride=self.stride_size,\n",
    "                    padding=self.padding_size,\n",
    "                    bias=False,\n",
    "                )\n",
    "            )\n",
    "            self.layers.append(nn.BatchNorm2d(num_features=self.out_channels))\n",
    "\n",
    "            self.in_channels = self.out_channels\n",
    "\n",
    "        self.layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        self.encoder_block = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.encoder_block(x)\n",
    "            x = x1 + x2\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def total_params(model):\n",
    "        if isinstance(model, EncoderBlock):\n",
    "            return sum(p.numel() for p in model.parameters())\n",
    "        else:\n",
    "            raise ValueError(\"Input must be an EncoderBlock\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Encoder Block for the TransUNet\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--in_channels\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Input channels to encode\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_channels\",\n",
    "        type=int,\n",
    "        default=256,\n",
    "        help=\"Output channels to decode\".capitalize(),\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    encoder_block = EncoderBlock(\n",
    "        in_channels=args.in_channels,\n",
    "        out_channels=args.out_channels,\n",
    "    )\n",
    "    images = torch.randn((16, 128, 64, 64))\n",
    "    assert (encoder_block(x=images).size()) == (16, 128 * 2, 64 // 2, 64 // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int = 512,\n",
    "        nheads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"relu\",\n",
    "        layer_norm_eps: float = 1e-05,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.dimension = dimension\n",
    "        self.nheads = nheads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.bias = bias\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            nheads=self.nheads, dimension=self.dimension\n",
    "        )\n",
    "        self.layer_norm1 = LayerNormalization(\n",
    "            dimension=self.dimension, layer_norm_eps=self.layer_norm_eps\n",
    "        )\n",
    "        self.layer_norm2 = LayerNormalization(\n",
    "            dimension=self.dimension, layer_norm_eps=self.layer_norm_eps\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(p=self.dropout)\n",
    "        self.dropout2 = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        self.feed_forward_network = FeedForwardNeuralNetwork(\n",
    "            dimension=self.dimension,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            activation=self.activation,\n",
    "            dropout=self.dropout,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise ValueError(\"Input must be a torch instance\".capitalize())\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.multi_head_attention(x=x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.add(x, residual)\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.feed_forward_network(x=x)\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.add(x, residual)\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transfomer = TransformerEncoderBlock(\n",
    "        dimension=512,\n",
    "        nheads=4,\n",
    "        dim_feedforward=1024,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        layer_norm_eps=1e-05,\n",
    "        bias=False,\n",
    "    )\n",
    "\n",
    "    images = torch.randn((1, 256, 512))\n",
    "    print(transfomer(x=images).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT: Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 256,\n",
    "        dimension: int = 512,\n",
    "        nheads: int = 8,\n",
    "        num_layers: int = 4,\n",
    "        dim_feedforward: int = 1024,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"relu\",\n",
    "        layer_norm_eps: float = 1e-05,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super(ViT, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.dimension = dimension\n",
    "        self.nheads = nheads\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.bias = bias\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            image_size=self.image_size,\n",
    "            patch_size=self.image_size // self.image_size,\n",
    "            dimension=self.dimension,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoderBlock(\n",
    "                    dimension=self.dimension,\n",
    "                    nheads=self.nheads,\n",
    "                    dim_feedforward=self.dim_feedforward,\n",
    "                    dropout=self.dropout,\n",
    "                    activation=self.activation,\n",
    "                    layer_norm_eps=self.layer_norm_eps,\n",
    "                    bias=self.bias,\n",
    "                )\n",
    "                for _ in tqdm(range(self.num_layers))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise ValueError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        for transformer in self.transformer:\n",
    "            x = transformer(x)\n",
    "\n",
    "        x = x.view(\n",
    "            x.size(0), x.size(-1), int(math.sqrt(x.size(1))), int(math.sqrt(x.size(1)))\n",
    "        )\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vit = ViT(\n",
    "        image_size=256,\n",
    "        dimension=512,\n",
    "        nheads=4,\n",
    "        num_layers=4,\n",
    "        dim_feedforward=1024,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        layer_norm_eps=1e-05,\n",
    "        bias=False,\n",
    "    )\n",
    "\n",
    "    images = torch.randn((16, 512, 16, 16))\n",
    "    print(vit(x=images).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int = 256, out_channels: int = 128):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.kernel_size = 1\n",
    "        self.stride_size = 1\n",
    "        self.padding_size = 1\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        self.upsample = nn.Upsample(\n",
    "            scale_factor=2, mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "\n",
    "        for _ in range(2):\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=self.in_channels,\n",
    "                    out_channels=self.out_channels,\n",
    "                    kernel_size=self.kernel_size * 3,\n",
    "                    stride=self.stride_size,\n",
    "                    padding=self.padding_size,\n",
    "                    bias=False,\n",
    "                )\n",
    "            )\n",
    "            self.layers.append(nn.BatchNorm2d(num_features=self.out_channels))\n",
    "            self.layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "            self.in_channels = self.out_channels\n",
    "\n",
    "        self.decoder_network = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = self.decoder_network(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def total_parameters(model):\n",
    "        if not isinstance(model, DecoderBlock):\n",
    "            raise ValueError(\"Input must be a DecoderBlock\".capitalize())\n",
    "\n",
    "        print(\"Total Parameter: \", total_params(model=model))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    decoder_block = DecoderBlock(\n",
    "        in_channels=256, out_channels=256//2\n",
    "    )\n",
    "\n",
    "    images = torch.randn((1, 256, 8, 8))\n",
    "\n",
    "    print(decoder_block(images).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransUNet: A combination of UNet and Transfomer ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransUNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_channels: int = 3,\n",
    "        image_size: int = 256,\n",
    "        nheads: int = 8,\n",
    "        num_layers: int = 4,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"relu\",\n",
    "        layer_norm_eps: float = 1e-05,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super(TransUNet, self).__init__()\n",
    "\n",
    "        self.image_channels = image_channels\n",
    "        self.image_size = image_size\n",
    "        self.nheads = nheads\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.bias = bias\n",
    "\n",
    "        warnings.warn(\n",
    "            \"Output channel configuration is determined based on image size:\\n\"\n",
    "            \"  - If image_size > 256, out_channels = (image_channels - 1) ** 8\\n\"\n",
    "            \"  - If image_size > 128, out_channels = (image_channels - 1) ** 6\\n\"\n",
    "            \"  - Otherwise, out_channels = (image_channels - 1) ** 5\"\n",
    "        )\n",
    "\n",
    "        if self.image_size > math.pow(2, 8):\n",
    "            self.out_channels = (self.image_channels - 1) ** 8\n",
    "        elif self.image_size > math.pow(2, 7):\n",
    "            self.out_channels = (self.image_channels - 1) ** 6\n",
    "        else:\n",
    "            self.out_channels = (self.image_channels - 1) ** 5\n",
    "\n",
    "        self.kernel_size = (self.image_channels * 2) + 1\n",
    "        self.stride_size = (self.kernel_size // self.kernel_size) + 1\n",
    "        self.padding_size = self.kernel_size // 2\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.image_channels,\n",
    "                out_channels=self.out_channels,\n",
    "                kernel_size=self.kernel_size,\n",
    "                stride=self.stride_size,\n",
    "                padding=self.padding_size,\n",
    "                bias=self.bias,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.in_channels = self.out_channels\n",
    "\n",
    "        self.encoder1 = EncoderBlock(\n",
    "            in_channels=self.in_channels, out_channels=self.in_channels * 2\n",
    "        )\n",
    "        self.encoder2 = EncoderBlock(\n",
    "            in_channels=self.in_channels * 2, out_channels=self.in_channels * 4\n",
    "        )\n",
    "        self.encoder3 = EncoderBlock(\n",
    "            in_channels=self.in_channels * 4, out_channels=self.in_channels * 8\n",
    "        )\n",
    "\n",
    "        self.decoder1 = DecoderBlock(\n",
    "            in_channels=self.in_channels * 8, out_channels=self.in_channels * 4\n",
    "        )\n",
    "        self.decoder2 = DecoderBlock(\n",
    "            in_channels=self.in_channels * 8, out_channels=self.in_channels * 2\n",
    "        )\n",
    "        self.decoder3 = DecoderBlock(\n",
    "            in_channels=self.in_channels * 4, out_channels=self.in_channels\n",
    "        )\n",
    "        self.decoder4 = DecoderBlock(\n",
    "            in_channels=self.in_channels * 2,\n",
    "            out_channels=self.in_channels // self.in_channels,\n",
    "        )\n",
    "\n",
    "        self.vit = ViT(\n",
    "            image_size=self.image_size,\n",
    "            dimension=self.in_channels * 8,\n",
    "            nheads=self.nheads,\n",
    "            num_layers=self.num_layers,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "            activation=self.activation,\n",
    "            layer_norm_eps=self.layer_norm_eps,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = self.conv1(x)\n",
    "\n",
    "            encoder1 = self.encoder1(x)\n",
    "            encoder2 = self.encoder2(encoder1)\n",
    "            encoder3 = self.encoder3(encoder2)\n",
    "\n",
    "            bottleneck = self.vit(x=encoder3)\n",
    "\n",
    "            decoder1 = self.decoder1(bottleneck)\n",
    "            decoder1 = torch.concat((decoder1, encoder2), dim=1)\n",
    "\n",
    "            decoder2 = self.decoder2(decoder1)\n",
    "            decoder2 = torch.concat((decoder2, encoder1), dim=1)\n",
    "\n",
    "            decoder3 = self.decoder3(decoder2)\n",
    "            decoder3 = torch.concat((decoder3, x), dim=1)\n",
    "\n",
    "            output = self.decoder4(decoder3)\n",
    "\n",
    "            return output\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = TransUNet(\n",
    "        image_channels=3,\n",
    "        image_size=256,\n",
    "        nheads=4,\n",
    "        num_layers=4,\n",
    "        dim_feedforward=1024,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        layer_norm_eps=1e-05,\n",
    "        bias=False,\n",
    "    )\n",
    "\n",
    "    images = torch.randn((1, 3, 256, 256))\n",
    "\n",
    "    segmented_images = model(x=images)\n",
    "\n",
    "    print(\"Output size:\", segmented_images.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataloader():\n",
    "    processed_path = path_names()[\"processed_data_path\"]\n",
    "\n",
    "    train_dataloader = os.path.join(processed_path, \"train_dataloader.pkl\")\n",
    "    valid_dataloader = os.path.join(processed_path, \"test_dataloader.pkl\")\n",
    "\n",
    "    train_dataloader = load_file(filename=train_dataloader)\n",
    "    valid_dataloader = load_file(filename=valid_dataloader)\n",
    "\n",
    "    return {\"train_dataloader\": train_dataloader, \"valid_dataloader\": valid_dataloader}\n",
    "\n",
    "\n",
    "def helper(**kwargs):\n",
    "    model: TransUNet = kwargs[\"model\"]\n",
    "    lr: float = kwargs[\"lr\"]\n",
    "    beta1: float = kwargs[\"beta1\"]\n",
    "    beta2: float = kwargs[\"beta2\"]\n",
    "    weight_decay: float = kwargs[\"weight_decay\"]\n",
    "    momentum: float = kwargs[\"momentum\"]\n",
    "    adam: bool = kwargs[\"adam\"]\n",
    "    SGD: bool = kwargs[\"SGD\"]\n",
    "    loss: str = kwargs[\"loss\"]\n",
    "    loss_smooth: float = kwargs[\"loss_smooth\"]\n",
    "    alpha_focal: float = kwargs[\"alpha_focal\"]\n",
    "    gamma_focal: float = kwargs[\"gamma_focal\"]\n",
    "    alpha_tversky: float = kwargs[\"alpha_tversky\"]\n",
    "    beta_tversky: float = kwargs[\"beta_tversky\"]\n",
    "\n",
    "    image_channels: int = config_files()[\"dataloader\"][\"image_channels\"]\n",
    "    image_size: int = config_files()[\"dataloader\"][\"image_size\"]\n",
    "    nheads: int = config_files()[\"TransUNet\"][\"nheads\"]\n",
    "    num_layers: int = config_files()[\"TransUNet\"][\"num_layers\"]\n",
    "    dim_feedforward: int = config_files()[\"TransUNet\"][\"dim_feedforward\"]\n",
    "    dropout: float = float(config_files()[\"TransUNet\"][\"dropout\"])\n",
    "    activation: str = config_files()[\"TransUNet\"][\"activation\"]\n",
    "    layer_norm_eps: float = float(config_files()[\"TransUNet\"][\"layer_norm_eps\"])\n",
    "    bias: bool = config_files()[\"TransUNet\"][\"bias\"]\n",
    "    \n",
    "\n",
    "    if model is None:\n",
    "        trans_unet = TransUNet(\n",
    "            image_channels=image_channels,\n",
    "            image_size=image_size,\n",
    "            nheads=nheads,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            layer_norm_eps=layer_norm_eps,\n",
    "            bias=bias\n",
    "        )\n",
    "    elif isinstance(model, TransUNet):\n",
    "        trans_unet = model\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. Expected TransUNet.\".capitalize())\n",
    "    \n",
    "    if adam:\n",
    "        optimizer = optim.Adam(params=trans_unet.parameters(), lr=lr, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "    elif SGD:\n",
    "        optimizer = optim.SGD(params=trans_unet.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    if loss == \"bce\" or loss == \"BCE\":\n",
    "        criterion = BCE(name=\"BCEWithLogitsLoss\")\n",
    "    elif loss == \"dice\":\n",
    "        criterion = DiceLoss(smooth=loss_smooth)\n",
    "    elif loss == \"focal\":\n",
    "        criterion = FocalLoss(alpha=alpha_focal, gamma=gamma_focal)\n",
    "    elif loss == \"jaccard\":\n",
    "        criterion = JaccardLoss(smooth=loss_smooth)\n",
    "    elif loss == \"tversky\":\n",
    "        criterion = TverskyLoss(alpha=alpha_tversky, beta=beta_tversky, smooth=loss_smooth)\n",
    "\n",
    "    return {\n",
    "        \"train_dataloader\": load_dataloader()[\"train_dataloader\"],\n",
    "        \"valid_dataloader\": load_dataloader()[\"valid_dataloader\"],\n",
    "        \"model\": trans_unet,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"criterion\": criterion,\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    adam, SGD = True, False\n",
    "    loss = \"bce\"\n",
    "    \"\"\"\n",
    "    \"dice\" | \"focal\" | \"jaccard\" | \"tversky\" | \"BCE\"\n",
    "    \"\"\"\n",
    "    init = helper(\n",
    "        model = None,\n",
    "        lr = 2e-4,\n",
    "        beta1 = 0.9,\n",
    "        beta2 = 0.999,\n",
    "        weight_decay = 1e-5,\n",
    "        momentum = 0.9,\n",
    "        adam = True,\n",
    "        SGD = False,\n",
    "        loss = \"bce\",\n",
    "        loss_smooth = 1e-6,\n",
    "        alpha_focal = 0.25,\n",
    "        gamma_focal = 2.0,\n",
    "        alpha_tversky = 0.5,\n",
    "        beta_tversky = 0.5\n",
    "    )\n",
    "\n",
    "    assert init[\"train_dataloader\"].__class__ == torch.utils.data.DataLoader\n",
    "    assert init[\"valid_dataloader\"].__class__ == torch.utils.data.DataLoader\n",
    "    assert init[\"model\"].__class__ == TransUNet\n",
    "    if adam:\n",
    "        assert init[\"optimizer\"].__class__ == torch.optim.Adam\n",
    "    elif SGD:\n",
    "        assert init[\"optimizer\"].__class__ == torch.optim.SGD\n",
    "    \n",
    "    if loss == \"bce\" or loss == \"BCE\":\n",
    "        assert init[\"criterion\"].__class__ == BCE\n",
    "    elif loss == \"dice\":\n",
    "        assert init[\"criterion\"].__class__ == DiceLoss\n",
    "    elif loss == \"focal\":\n",
    "        assert init[\"criterion\"].__class__ == FocalLoss\n",
    "    elif loss == \"tversky\":\n",
    "        assert init[\"criterion\"].__class__ == TverskyLoss\n",
    "    elif loss == \"jaccard\":\n",
    "        assert init[\"criterion\"].__class__ == JaccardLoss\n",
    "    else:\n",
    "        raise ValueError(\"Invalid loss function. Expected one of: bce, dice, focal, jaccard, tversky.\".capitalize())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
